{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Classification of News Articles from Different Sources\n",
    "\n",
    "This is Part 2 of my AM216 project on simulating and differentiating the Harvard Crimson and the Harvard Gazette. Please go to Part 1 for the intro, scraping, data generation with RNN, and exploratory analyses. \n",
    "\n",
    "There is lots of literature on classifying news articles by category (Sports, Arts, Tech, etc.) but I couldn't find any research on classifying news articles from different news sources. However, I think this could be very interesting because it shows how different different news sources may be despite discussing many of the same things. A human could easily read an article and tell you whether it belongs in Sports or Tech, for example. But unless they know the tone of the papers very well they would have a much harder time telling you whether the article came from the Harvard Gazette or the Harvard Crimson. Would a computer be able to distinguish them easily?\n",
    "\n",
    "This part classifies news articles by tokenizing, word-stemming/lemmatizing the data into a set of words with NLTK, vectorizing with TFIDF, and fitting an SVM. I used [this tutorial](https://medium.com/@bedigunjit/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34). \n",
    "\n",
    "While one can classify text with pretty much any classification method, as seen [here](https://github.com/miguelfzafra/Latest-News-Classifier/blob/master/0.%20Latest%20News%20Classifier/04.%20Model%20Training/12.%20Best%20Model%20Selection.ipynb), it's been shown that SVMs generally perform the best. ![](classification.png)\n",
    "\n",
    "To begin, I import packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/terry/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/terry/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /Users/terry/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random seed for reproducible results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harvard Gazette vs. Harvard Crimson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building dataframes with all Gazette and Crimson articles using my original and my generated data (stored in .txt files in Part 1). I labeled Crimson articles as 1 and the Gazette articles as 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus_gen=pd.DataFrame(columns=['text', 'label'])\n",
    "\n",
    "for i in range(100):\n",
    "    with open(\"articles/gen/crimson\"+str(i)+\".txt\", \"r\") as file:\n",
    "        text=file.read()\n",
    "        Corpus_gen=Corpus_gen.append({'text': text, 'label': 1}, ignore_index=True)\n",
    "    \n",
    "for i in range(144):\n",
    "    with open(\"articles/gen/gazette\"+str(i)+\".txt\", \"r\") as file:\n",
    "        text=file.read()\n",
    "        Corpus_gen=Corpus_gen.append({'text': text, 'label': 0}, ignore_index=True)\n",
    " \n",
    "\n",
    "Corpus_og=pd.DataFrame(columns=['text', 'label'])\n",
    "\n",
    "for i in range(47):\n",
    "    with open(\"articles/og/crimson\"+str(i+100)+\".txt\", \"r\") as file:\n",
    "        text=file.read()\n",
    "        Corpus_og=Corpus_og.append({'text': text, 'label': 1}, ignore_index=True)\n",
    "    \n",
    "for i in range(24):\n",
    "    with open(\"articles/og/gazette\"+str(i+144)+\".txt\", \"r\") as file:\n",
    "        text=file.read()\n",
    "        Corpus_og=Corpus_og.append({'text': text, 'label': 0}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I clean my data by changing everything to lowercase, splitting it into words, and doing word stemming/lemmatizing. Word stemming/lemmatizing essentially breaks words down to their roots and grouping variant forms of a word. \"Run\" and \"running\", for example, would be interpreted similarly.\n",
    "\n",
    "Then, I split the data into a train and a test set. I do the generated data and the original data separately because my generated data is obviously less accurate than the original data and I want more of my better data to go in the test set, since it's been shown that noisy data in training is fine as long as the test set is accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the generated data\n",
    "\n",
    "# Change all the text to lower case\n",
    "Corpus_gen['text'] = [entry.lower() for entry in Corpus_gen['text']]\n",
    "\n",
    "# Tokenization : In this each entry in the corpus will be broken into set of words\n",
    "Corpus_gen['text']= [word_tokenize(entry) for entry in Corpus_gen['text']]\n",
    "\n",
    "# Remove stop words, non-numeric and perfom word stemming/lemmatizing\n",
    "\n",
    "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "\n",
    "for index,entry in enumerate(Corpus_gen['text']):\n",
    "    # Declaring empty list to store the words that follow the rules for this step\n",
    "    Final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for stop words and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "    Corpus_gen.loc[index,'text_final'] = str(Final_words)\n",
    "\n",
    "\n",
    "# Split the into train and test set, since this is generated data 85% will go into the training set\n",
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Corpus_gen['text_final'],Corpus_gen['label'],test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all the text to lower case\n",
    "Corpus_og['text'] = [entry.lower() for entry in Corpus_og['text']]\n",
    "\n",
    "# Tokenization\n",
    "Corpus_og['text']= [word_tokenize(entry) for entry in Corpus_og['text']]\n",
    "\n",
    "# Remove stop words, non-numeric and perfom word stemming/lemmatizing\n",
    "\n",
    "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "\n",
    "for index,entry in enumerate(Corpus_og['text']):\n",
    "    # Declaring empty list to store the words that follow the rules for this step\n",
    "    Final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for stop words and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "    Corpus_og.loc[index,'text_final'] = str(Final_words)\n",
    "\n",
    "\n",
    "# Split the model into train and test data set, since this is the original data 30% will go into the test set\n",
    "Train_Xadd, Test_Xadd, Train_Yadd, Test_Yadd = model_selection.train_test_split(Corpus_og['text_final'],Corpus_og['label'],test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I combine my original and generated data to make my final train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X=Train_X.append(Train_Xadd)\n",
    "Test_X=Test_X.append(Test_Xadd)\n",
    "Train_Y=Train_Y.append(Train_Yadd)\n",
    "Test_Y=Test_Y.append(Test_Yadd)\n",
    "\n",
    "Corpus=Corpus_gen.append(Corpus_og)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making my data numeric and vectorizing with TFIDF: Term Frequency * Inverse Document Frequency. This is done to find how important a word in document is in comparison to the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode the target variable: transforming categorical data of string type in the data set into numerical values\n",
    "Encoder = LabelEncoder()\n",
    "Train_Y = Encoder.fit_transform(Train_Y)\n",
    "Test_Y = Encoder.fit_transform(Test_Y)\n",
    "\n",
    "# Vectorize the words by using TF-IDF Vectorizer\n",
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(Corpus['text_final'])\n",
    "\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, using an SVM classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  94.91525423728814\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "95% accuracy is pretty good! That suggests Harvard Crimson articles do differ from Harvard Gazette articles, despite all being about the same subject, Harvard. There must be differences in the language used, whether unintentional due to different writers or intentional in the tone/POV they want to convey.\n",
    "\n",
    "## Comparing Generated vs. Original Data\n",
    "\n",
    "I thought it would be interesting to try and classify my generated vs. my original data, since it gives a metric on how well my RNN in Part 1 performed in generation. Of course, my loss in Part 1 is also such a metric, but how well an SVM performs is easier to interpret. Ideally, I'd want close to 50% accuracy since that means my generated and original articles would be so close the SVM can't do better than random guessing.\n",
    "\n",
    "Building my combined dataframe from the .txt files with generated articles labeled as 0 and original articles labeled as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus_comp=pd.DataFrame(columns=['text', 'label'])\n",
    "\n",
    "for i in range(100):\n",
    "    with open(\"articles/gen/crimson\"+str(i)+\".txt\", \"r\") as file:\n",
    "        text=file.read()\n",
    "        Corpus_comp=Corpus_comp.append({'text': text, 'label': 0}, ignore_index=True)\n",
    "    \n",
    "for i in range(144):\n",
    "    with open(\"articles/gen/gazette\"+str(i)+\".txt\", \"r\") as file:\n",
    "        text=file.read()\n",
    "        Corpus_comp=Corpus_comp.append({'text': text, 'label': 0}, ignore_index=True)\n",
    "\n",
    "for i in range(47):\n",
    "    with open(\"articles/og/crimson\"+str(i+100)+\".txt\", \"r\") as file:\n",
    "        text=file.read()\n",
    "        Corpus_comp=Corpus_comp.append({'text': text, 'label': 1}, ignore_index=True)\n",
    "    \n",
    "for i in range(24):\n",
    "    with open(\"articles/og/gazette\"+str(i+144)+\".txt\", \"r\") as file:\n",
    "        text=file.read()\n",
    "        Corpus_comp=Corpus_comp.append({'text': text, 'label': 1}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same data cleaning and 80%-20% train-test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change all the text to lower case\n",
    "Corpus_comp['text'] = [entry.lower() for entry in Corpus_comp['text']]\n",
    "\n",
    "# Tokenization\n",
    "Corpus_comp['text']= [word_tokenize(entry) for entry in Corpus_comp['text']]\n",
    "\n",
    "# Remove stop words, non-numeric and perfom word stemming/lemmatization\n",
    "\n",
    "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "\n",
    "for index,entry in enumerate(Corpus_comp['text']):\n",
    "    # Declaring empty list to store the words that follow the rules for this step\n",
    "    Final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for stop words and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "    Corpus_comp.loc[index,'text_final'] = str(Final_words)\n",
    "\n",
    "\n",
    "# Split the model into train and test\n",
    "Train_Xc, Test_Xc, Train_Yc, Test_Yc = model_selection.train_test_split(Corpus_comp['text_final'],Corpus_comp['label'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, label encoding and vectorizing with TFIDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encode the target variable\n",
    "Encoder = LabelEncoder()\n",
    "Train_Yc = Encoder.fit_transform(Train_Yc)\n",
    "Test_Yc = Encoder.fit_transform(Test_Yc)\n",
    "\n",
    "# Vectorize the words by using TF-IDF Vectorizer\n",
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(Corpus_comp['text_final'])\n",
    "\n",
    "Train_Xc_Tfidf = Tfidf_vect.transform(Train_Xc)\n",
    "Test_Xc_Tfidf = Tfidf_vect.transform(Test_Xc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  80.95238095238095\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_Xc_Tfidf,Train_Yc)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_Xc_Tfidf)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Yc)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM is much less accurate than when we compared Crimson and Gazette articles. This makes sense since our RNNs trained on the Crimson vs the Gazette should generate articles with similar vocabularies as the Crimson and the Gazette themselves! However, we still obtain an accuracy quite a bit higher than 50% meaning our generated articles are still pretty distinguishable from the original ones (which a reader would be able to tell, also). However, this also means that about a fifth of the time the SVM can't tell the difference between generated and original articles. \n",
    "\n",
    "A way to continue this project may be to try and develop a text GAN, where the classifier and the generator train each other. This is more difficult than image GANs because RNNs work in a chain so it can be hard to integrate what is already technically a combination of networks. However, there has been research published on this. \n",
    "\n",
    "## Beyond the Gazette vs. the Crimson\n",
    "\n",
    "The Harvard Gazette and the Harvard Crimson are both about Harvard, and can be distinguished. Can this be applied to other news sources? I tested SVM classification on the New York Times and and Wall Street Journal. Both are two very popular newspapers based in New York, so similar subjects should be covered and a human might not be able to classify the articles. \n",
    "\n",
    "Scraping with the Python newspaper package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NYT vs WSJ\n",
    "import newspaper\n",
    "\n",
    "Corpus_other=pd.DataFrame(columns=['text', 'label'])\n",
    "\n",
    "nyt_paper = newspaper.build('http://nytimes.com', memoize_articles=False)\n",
    "wsj_paper = newspaper.build('http://wsj.com', memoize_articles=False)  \n",
    "\n",
    "for article in nyt_paper.articles:\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    Corpus_other=Corpus_other.append({'text': article.text, 'label': 1}, ignore_index=True)\n",
    "\n",
    "for article in wsj_paper.articles:\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    Corpus_other=Corpus_other.append({'text': article.text, 'label': 0}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n",
      "261\n"
     ]
    }
   ],
   "source": [
    "print(len(nyt_paper.articles))\n",
    "print(len(wsj_paper.articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, I don't need to generate data because plenty of original articles can be scraped from their category pages and RSS feeds (137 articles from the New York Times and 261 articles from the Wall Street Journal).\n",
    "\n",
    "The same classification process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  91.25\n"
     ]
    }
   ],
   "source": [
    "# Change all the text to lower case\n",
    "Corpus_other['text'] = [entry.lower() for entry in Corpus_other['text']]\n",
    "\n",
    "# Tokenization\n",
    "Corpus_other['text']= [word_tokenize(entry) for entry in Corpus_other['text']]\n",
    "\n",
    "# Remove stop words, non-numeric and perfom word stemming/lemmatization\n",
    "\n",
    "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "\n",
    "for index,entry in enumerate(Corpus_other['text']):\n",
    "    # Declaring empty list to store the words that follow the rules for this step\n",
    "    Final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for stop words and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "    Corpus_other.loc[index,'text_final'] = str(Final_words)\n",
    "\n",
    "\n",
    "# Split the model into train and test data set\n",
    "Train_Xo, Test_Xo, Train_Yo, Test_Yo = model_selection.train_test_split(Corpus_other['text_final'],Corpus_other['label'],test_size=0.2)\n",
    "\n",
    "# Label encoding\n",
    "Encoder = LabelEncoder()\n",
    "Train_Yo = Encoder.fit_transform(Train_Yo)\n",
    "Test_Yo = Encoder.fit_transform(Test_Yo)\n",
    "\n",
    "# Vectorize the words with TF-IDF\n",
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(Corpus['text_final'])\n",
    "\n",
    "Train_Xo_Tfidf = Tfidf_vect.transform(Train_Xo)\n",
    "Test_Xo_Tfidf = Tfidf_vect.transform(Test_Xo)\n",
    "\n",
    "# SVM Classifier\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_Xo_Tfidf,Train_Yo)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_Xo_Tfidf)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Yo)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 90%+ classification was still pretty good using all original articles from two prestigious New York-based newspapers. This means that there is a difference in the kinds of words used here too, whether unintentional due to different writers or intentional due to tone/POV they wanted to convey.\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "Crimson and Gazette articles generated with RNNs can represent the original articles to some extent, as it was relatively difficult to classify generated and original articles. A text GAN might be something to explore, to generate even better articles. \n",
    "\n",
    "Though articles from different papers may be about similar subjects and of similar lengths and sentiments, they are still quite easily distinguished with a SVM. The differences in words used may be due to difference in authors and the tone they try to convey. This can easily be extended to classifying articles from multiple papers, with more labels. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
